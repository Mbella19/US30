"""
Gymnasium Trading Environment for the Sniper Agent.

Features:
- Multi-Discrete action space: [Direction (3), Size (4)]
- Frozen Market Analyst provides context vectors
- Reward shaping: PnL, transaction costs, FOMO penalty, chop avoidance
- Normalized observations (prevents scale inconsistencies)

Optimized for M2 Silicon with all float32 operations.
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import torch
from typing import Dict, Tuple, Optional, Any
import pandas as pd
import gc
import logging

# Import config for default values
from config.settings import config as default_config

logger = logging.getLogger(__name__)


class TradingEnv(gym.Env):
    """
    Trading environment for the PPO Sniper Agent.

    Action Space:
        Multi-Discrete([3, 4]):
        - Direction: 0=Flat/Exit, 1=Long, 2=Short
        - Size: 0=0.25x, 1=0.5x, 2=0.75x, 3=1.0x

    Observation Space:
        Box containing:
        - Context vector from frozen Analyst (context_dim)
        - Position state: [position, entry_price_norm, unrealized_pnl_norm]
        - Market features: [atr, chop, adx, regime, sma_distance]

    Reward:
        Base PnL (pips) × position_size
        - Transaction cost when opening
        - FOMO penalty when flat during momentum
        - Chop penalty when holding in ranging market
    """

    metadata = {'render_modes': ['human']}

    # Position sizing multipliers
    POSITION_SIZES = (0.5, 1.0, 1.5, 2.0)  # Doubled for aggressive risk-taking
    # Reward: pure continuous mark-to-market PnL delta (no exit "banking").
    # The agent receives PnL changes as they occur, aligning rewards with the
    # equity curve and avoiding incentive to prematurely close to "lock in" a
    # large pending exit reward.
    # Holding bonus (anti-reward-hacking):
    # - Gated: only after trade is net profitable beyond entry costs + buffer.
    # - Progress-based: pays only when reaching new profit milestones (high-water).
    # - Capped: max bonus per trade is a small fraction of entry cost (in reward units).
    HOLDING_BONUS_AMOUNT = 0.01
    HOLDING_BONUS_CAP_FRACTION_OF_ENTRY_COST = 0.2
    HOLDING_BONUS_BUFFER_FRACTION_OF_ENTRY_COST = 1.0
    HOLDING_BONUS_MILESTONE_PIPS = 5.0

    def __init__(
        self,
        data_5m: np.ndarray,
        data_15m: np.ndarray,
        data_45m: np.ndarray,
        close_prices: np.ndarray,
        market_features: np.ndarray,
        analyst_model: Optional[torch.nn.Module] = None,
        context_dim: int = 64,
        lookback_5m: int = 48,
        lookback_15m: int = 16,
        lookback_45m: int = 6,
        pip_value: float = 1.0,       # US30: 1 point = 1.0 price movement
        lot_size: float = 1.0,        # US30 CFD lot size
        point_multiplier: float = 1.0, # Dollar per point per lot
        spread_pips: float = 50.0,     # US30 spread with buffer
        slippage_pips: float = 0.0,   # US30 slippage
        fomo_penalty: float = 0.0,  # DEPRECATED - now using opportunity cost
        chop_penalty: float = 0.0,    # Disabled for stability
        fomo_threshold_atr: float = 2.0,  # Trigger on >4x ATR moves over lookback window
        fomo_lookback_bars: int = 20,     # Check move over 10 bars
        chop_threshold: float = 80.0,     # Only extreme chop triggers penalty
        max_steps: int = 500,         # ~1 week for rapid regime cycling
        reward_scaling: float = 0.01,  # US30: 1.0 per 100 points (was 1.0 per pip for EURUSD)
        # Symmetric PnL Scaling (direction compensation handled separately)
        profit_scaling: float = 0.01,   # Symmetric with loss_scaling
        loss_scaling: float = 0.01,     # 1.0x for losses
        # Direction compensation REMOVED - symmetric multipliers
        short_profit_multiplier: float = 1.0,  # No boost - symmetric
        short_loss_multiplier: float = 1.0,    # No reduction - symmetric
        # Alpha-Based Reward - rewards OUTPERFORMANCE vs market, not absolute PnL
        use_alpha_reward: bool = True,   # Use relative returns instead of absolute PnL
        alpha_baseline_exposure: float = 0.3,  # 30% baseline - preserves 70% raw signal
        trade_entry_bonus: float = 0.0,    # DISABLED - alpha asymmetry provides trading incentive
        holding_bonus: float = 0.0,        # DISABLED - was causing reward inflation
        device: Optional[torch.device] = None,
        market_feat_mean: Optional[np.ndarray] = None,  # Pre-computed from training data
        market_feat_std: Optional[np.ndarray] = None,    # Pre-computed from training data
        pre_windowed: bool = True,  # FIXED: If True, data is already windowed (start_idx=0)
        # Risk Management
        sl_atr_multiplier: float = 2.0, # SL at 1x ATR for tight stops
        tp_atr_multiplier: float = 6.0, # TP at 3x ATR (1:3 R/R ratio)
        use_stop_loss: bool = True,     # Enable/disable stop-loss
        use_take_profit: bool = True,   # Enable/disable take-profit
        # Regime-balanced sampling
        regime_labels: Optional[np.ndarray] = None,  # 0=Bullish, 1=Ranging, 2=Bearish
        use_regime_sampling: bool = True,  # Sample episodes balanced across regimes
        # Volatility Sizing (Dollar-based risk)
        volatility_sizing: bool = True,  # Scale position size to maintain constant dollar risk
        risk_per_trade: float = 100.0,   # Dollar risk per trade (e.g., $100 per trade)
        # Classification mode
        num_classes: int = 2,  # Binary (2) vs multi-class (3) - affects observation size
        # Analyst Alignment
        enforce_analyst_alignment: bool = False,  # If True, restrict actions to analyst direction
        # Pre-computed Analyst outputs (for sequential context)
        precomputed_analyst_cache: Optional[dict] = None,  # {'contexts': np.ndarray, 'probs': np.ndarray}
        # OHLC data for visualization (real candle data)
        ohlc_data: Optional[np.ndarray] = None,  # Shape: (n_samples, 4) with [open, high, low, close]
        timestamps: Optional[np.ndarray] = None,  # Optional timestamps for real time axis
        noise_level: float = 0.02,  # Reduced noise
        # Sparse Rewards Mode
        use_sparse_rewards: bool = False,  # DISABLED - causes mode collapse
        # Loss Tolerance Buffer
        loss_tolerance_atr: float = 0.5,  # Allow this much ATR drawdown before sparse mode kicks in
        # Forced Minimum Hold Time
        min_hold_bars: int = 0,  # Disabled (use early_exit_penalty instead)
        early_exit_profit_atr: float = 3.0,  # Allow early exit if profit > this ATR multiple
        break_even_atr: float = 2.0,  # Move SL to break-even when profit reaches this ATR
        # Early Exit Penalty - discourages scalping
        early_exit_penalty: float = 0.0,  # Disabled
        min_bars_before_exit: int = 10,    # Minimum bars before penalty-free exit
        # Underwater Decay Penalty - penalizes holding losing trades
        underwater_penalty_coef: float = 0.05,   # Strong penalty for holding losers
        underwater_threshold_atr: float = 1.0,    # Only penalize losses beyond this threshold
        # Opportunity Cost parameters
        opportunity_cost_multiplier: float = 1.0, # Multiplier for missed profit when flat during moves
        opportunity_cost_cap: float = 0.05,        # Maximum penalty per bar for opportunity cost
        # Full Eyes Features
        returns: Optional[np.ndarray] = None, # Recent 5m log-returns
        agent_lookback_window: int = 0, # How many return bars to see
    # Toggle Analyst usage (if False, agent uses only market features)
        use_analyst: bool = True,  # Re-enabled - agent uses analyst context
        # Rolling window warmup data (market features from before start_idx)
        rolling_lookback_data: Optional[np.ndarray] = None,
        rolling_norm_min_samples: int = None,  # Defaults to config.trading.rolling_norm_min_samples
        rolling_window_size: int = None,       # Defaults to config.normalization.rolling_window_size
        clip_value: float = None,              # Defaults to config.normalization.clip_value
        # v37 Trade Filters (block trades during problematic times)
        use_trade_filters: bool = False,  # Enable/disable trade filtering
        blocked_hours: tuple = (18, 19),  # UTC hours to block (6-7 PM = low liquidity)
        blocked_days: tuple = (6,),  # Days to block (6=Sunday)
        # v37 OOD Position Sizing (aggressive reduction based on ood_score)
        ood_size_reduction_factor: float = 0.8,  # How aggressively to reduce (0.8 = 20% to 100%)
        min_position_size_ratio: float = 0.2,  # Minimum position size (20% of normal)
    ):
        """
        Initialize the trading environment.

        Args:
            data_5m: 5-minute feature data [num_samples, lookback_5m, features] (base timeframe)
            data_15m: 15-minute feature data [num_samples, lookback_15m, features]
            data_45m: 45-minute feature data [num_samples, lookback_45m, features]
            close_prices: Close prices for PnL calculation [num_samples]
            market_features: Additional features [num_samples, n_features]
                            Expected: [atr, chop, adx, regime, sma_distance]
            analyst_model: Frozen Market Analyst for context generation
            context_dim: Dimension of context vector
            lookback_*: Lookback windows for each timeframe
            spread_pips: Transaction cost in pips
            fomo_penalty: Penalty for being flat during momentum
            chop_penalty: Penalty for holding in ranging market
            fomo_threshold_atr: ATR multiplier for FOMO detection
            chop_threshold: Choppiness index threshold
            max_steps: Maximum steps per episode
            reward_scaling: Scale factor for PnL rewards (0.1 = ±20 pips becomes ±2.0)
                           This balances PnL with penalties for "Sniper" behavior.
            device: Torch device for analyst inference
            noise_level: Std dev of Gaussian noise to add to observations (0.0 = disabled)
            returns: Raw log-returns series for Agent's peripheral vision
            agent_lookback_window: Number of return steps to observe
        """
        super().__init__()

        # Anti-Overfitting: Gaussian Noise
        self.noise_level = noise_level
        if self.noise_level > 0:
            logger.debug(f"Gaussian Noise Injection ENABLED: sigma={self.noise_level}")
        # Avoid unnecessary copies for large arrays (critical for memory use).
        self.data_5m = data_5m.astype(np.float32, copy=False)
        self.data_15m = data_15m.astype(np.float32, copy=False)
        self.data_45m = data_45m.astype(np.float32, copy=False)
        self.close_prices = close_prices.astype(np.float32, copy=False)
        self.market_features = market_features.astype(np.float32, copy=False)

        # Skip normalization for volatility context if present.
        self._market_skip_norm_idx = np.array([], dtype=np.int64)
        if len(self.market_features.shape) > 1:
            try:
                from src.live.bridge_constants import MARKET_FEATURE_COLS
            except Exception:
                MARKET_FEATURE_COLS = None
            if MARKET_FEATURE_COLS and "atr_context" in MARKET_FEATURE_COLS:
                base_len = len(MARKET_FEATURE_COLS)
                n_features = self.market_features.shape[1]
                ctx_idx = MARKET_FEATURE_COLS.index("atr_context")
                if n_features == base_len:
                    self._market_skip_norm_idx = np.array([ctx_idx], dtype=np.int64)
                elif n_features % base_len == 0:
                    blocks = n_features // base_len
                    self._market_skip_norm_idx = np.array(
                        [ctx_idx + base_len * i for i in range(blocks)],
                        dtype=np.int64
                    )
        
        # New "Full Eyes" data
        self.returns = returns.astype(np.float32, copy=False) if returns is not None else None
        self.agent_lookback_window = agent_lookback_window
        
        # OHLC data for visualization (real candle data)
        self.ohlc_data = ohlc_data  # Shape: (n_samples, 4) = [open, high, low, close]
        self.timestamps = timestamps  # Unix timestamps for real time axis

        # Analyst model
        self.analyst = analyst_model
        self.device = device or torch.device('cpu')
        self.context_dim = context_dim

        # Lookback windows
        self.lookback_5m = lookback_5m
        self.lookback_15m = lookback_15m
        self.lookback_45m = lookback_45m

        # Trading parameters
        self.pip_value = pip_value  # US30: 1.0 (1 point = 1.0 price movement)
        self.lot_size = lot_size    # US30 CFD lot size (1.0)
        self.point_multiplier = point_multiplier  # Dollar per point per lot (1.0)
        self.spread_pips = spread_pips
        self.slippage_pips = slippage_pips  # Realistic execution slippage
        self.fomo_penalty = fomo_penalty
        self.chop_penalty = chop_penalty
        self.fomo_threshold_atr = fomo_threshold_atr
        self.fomo_lookback_bars = fomo_lookback_bars  # Multi-bar FOMO check
        self.chop_threshold = chop_threshold
        self.max_steps = max_steps
        self.reward_scaling = reward_scaling  # Scale PnL to balance with penalties
        # PnL Scaling (symmetric)
        self.profit_scaling = profit_scaling
        self.loss_scaling = loss_scaling
        # Direction-Compensated Rewards
        self.short_profit_multiplier = short_profit_multiplier
        self.short_loss_multiplier = short_loss_multiplier
        # Alpha-Based Reward
        self.use_alpha_reward = use_alpha_reward
        self.alpha_baseline_exposure = alpha_baseline_exposure
        self.trade_entry_bonus = trade_entry_bonus  # Bonus for opening positions
        self.holding_bonus = holding_bonus  # Bonus for staying in profitable trades

        # Risk Management - Stop-Loss and Take-Profit
        self.sl_atr_multiplier = sl_atr_multiplier
        self.tp_atr_multiplier = tp_atr_multiplier
        self.use_stop_loss = use_stop_loss
        self.use_take_profit = use_take_profit
        
        # Volatility Sizing (Dollar-based risk)
        self.volatility_sizing = volatility_sizing
        self.risk_per_trade = risk_per_trade  # Dollar risk per trade
        
        # Analyst Alignment
        self.enforce_analyst_alignment = enforce_analyst_alignment
        self.current_probs = None  # Store for action masking
        
        # Sparse Rewards - only reward on trade exit
        self.use_sparse_rewards = use_sparse_rewards
        # Loss Tolerance Buffer
        self.loss_tolerance_atr = loss_tolerance_atr
        # Forced Minimum Hold Time
        self.min_hold_bars = min_hold_bars
        self.early_exit_profit_atr = early_exit_profit_atr  # Allow early exit if profit > this ATR
        self.break_even_atr = break_even_atr  # Move SL to break-even at this profit
        self.break_even_activated = False  # Track if break-even has been triggered
        self.entry_idx = 0  # Track when position was opened
        # Early Exit Penalty
        self.early_exit_penalty = early_exit_penalty
        self.min_bars_before_exit = min_bars_before_exit
        # Underwater Decay Penalty
        self.underwater_penalty_coef = underwater_penalty_coef
        self.underwater_threshold_atr = underwater_threshold_atr
        # Opportunity Cost
        self.opportunity_cost_multiplier = opportunity_cost_multiplier
        self.opportunity_cost_cap = opportunity_cost_cap

        # v37 Trade Filters
        self.use_trade_filters = use_trade_filters
        self.blocked_hours = set(blocked_hours)
        self.blocked_days = set(blocked_days)
        # v37 OOD Position Sizing
        self.ood_size_reduction_factor = ood_size_reduction_factor
        self.min_position_size_ratio = min_position_size_ratio

        # Calculate valid range FIRST (needed for regime indices)
        # FIXED: If pre_windowed=True, data is already trimmed by prepare_env_data
        # so start_idx should be 0 (no double offset)
        if pre_windowed:
            self.start_idx = 0
        else:
            # Only compute start_idx if using raw DataFrames (create_env_from_dataframes)
            # Subsample ratios: 15m = 3x base (5m), 45m = 9x base (5m)
            self.start_idx = max(lookback_5m, lookback_15m * 3, lookback_45m * 9)
        
        self.end_idx = len(close_prices) - 1
        self.n_samples = self.end_idx - self.start_idx
        
        # Regime-balanced sampling (AFTER start_idx/end_idx are set)
        self.use_regime_sampling = use_regime_sampling and regime_labels is not None
        if regime_labels is not None:
            self.regime_labels = regime_labels.astype(np.int32)
            # Pre-compute indices for each regime (0=Bullish, 1=Ranging, 2=Bearish)
            self.regime_indices = {
                0: np.where(self.regime_labels == 0)[0],  # Bullish
                1: np.where(self.regime_labels == 1)[0],  # Ranging
                2: np.where(self.regime_labels == 2)[0],  # Bearish
            }
            # Filter to valid range for episode starts
            max_start = max(self.start_idx + 1, self.end_idx - max_steps)
            for regime in self.regime_indices:
                valid = self.regime_indices[regime]
                valid = valid[(valid >= self.start_idx) & (valid < max_start)]
                self.regime_indices[regime] = valid
            logger.debug(f"Regime sampling enabled: Bullish={len(self.regime_indices[0])}, "
                         f"Ranging={len(self.regime_indices[1])}, Bearish={len(self.regime_indices[2])}")
        else:
            self.regime_labels = None
            self.regime_indices = None

        # Action space: Multi-Discrete([direction, size])
        # Direction: 0=Flat, 1=Long, 2=Short
        # Size: 0=0.25, 1=0.5, 2=0.75, 3=1.0
        self.action_space = spaces.MultiDiscrete([3, 4])

        # Store num_classes for observation construction
        self.num_classes = num_classes

        # Store use_analyst flag
        self.use_analyst = use_analyst

        # Observation space
        # Context vector + position state (3) + market features (5) + analyst_metrics
        # Binary (2 classes): [p_down, p_up, edge, confidence, uncertainty] = 5
        # Multi-class (3 classes): [p_down, p_neutral, p_up, edge, confidence, uncertainty] = 6
        n_market_features = market_features.shape[1] if len(market_features.shape) > 1 else 5

        # Adjust observation dimensions based on use_analyst flag
        if self.use_analyst and (analyst_model is not None or precomputed_analyst_cache is not None):
            effective_context_dim = context_dim
            analyst_metrics_dim = 5 if num_classes == 2 else 6
        else:
            effective_context_dim = 0
            analyst_metrics_dim = 0
            logger.debug("Analyst DISABLED - agent using market features only")

        # Obs Dim = Context + Position(4) + Market + Analyst + SL/TP(2) + Hold(4) + Returns
        # Added 4 hold features: profit_progress, dist_to_tp_pct, momentum_aligned, session_progress
        obs_dim = effective_context_dim + 4 + n_market_features + analyst_metrics_dim + 2 + 4 + self.agent_lookback_window

        # Store effective dimensions for _get_observation
        self.effective_context_dim = effective_context_dim
        self.analyst_metrics_dim = analyst_metrics_dim

        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(obs_dim,),
            dtype=np.float32
        )

        # CRITICAL: Normalize market features to prevent scale inconsistencies!
        # Market features (ATR ~0.001, CHOP 0-100, ADX 0-100) have vastly different scales.
        # FIXED: Use pre-computed stats from training data to prevent look-ahead bias.
        if market_feat_mean is not None and market_feat_std is not None:
            # Use pre-computed statistics (no look-ahead bias)
            self.market_feat_mean = market_feat_mean.astype(np.float32)
            self.market_feat_std = market_feat_std.astype(np.float32)
        elif len(market_features.shape) > 1 and market_features.shape[1] > 0:
            # Fallback: compute from provided data (should only be used with training data)
            self.market_feat_mean = market_features.mean(axis=0).astype(np.float32)
            self.market_feat_std = market_features.std(axis=0).astype(np.float32)
            # Prevent division by zero for constant features
            self.market_feat_std = np.where(self.market_feat_std > 1e-8,
                                           self.market_feat_std,
                                           1.0).astype(np.float32)
        else:
            self.market_feat_mean = None
            self.market_feat_std = None

        # Rolling window normalization for non-stationary financial data
        # OPTIMIZED: Use numpy circular buffer with running sums for O(1) updates
        # Resolve config defaults if not provided
        rolling_norm_min_samples = rolling_norm_min_samples if rolling_norm_min_samples is not None else default_config.trading.rolling_norm_min_samples
        rolling_window_size = rolling_window_size if rolling_window_size is not None else default_config.normalization.rolling_window_size
        clip_value = clip_value if clip_value is not None else default_config.normalization.clip_value

        self.rolling_window_size = rolling_window_size
        num_features = n_market_features
        self.use_rolling_norm = True  # Can be disabled for comparison
        self.rolling_min_samples = max(1, int(rolling_norm_min_samples))
        self.clip_value = clip_value  # Clipping value for normalized features
        
        # Circular buffer for rolling window (much faster than deques)
        self.rolling_buffer = np.zeros((self.rolling_window_size, num_features), dtype=np.float32)
        self.rolling_idx = 0  # Current position in circular buffer
        self.rolling_count = 0  # Number of samples added (up to window_size)
        
        # Running sums for O(1) mean/std calculation
        self.rolling_sum = np.zeros(num_features, dtype=np.float64)  # Use float64 for precision
        self.rolling_sum_sq = np.zeros(num_features, dtype=np.float64)
        
        # Store pre-start lookback data for warmup (injected by factory)
        self.rolling_lookback_data = rolling_lookback_data

        # Episode state
        self.current_idx = self.start_idx
        self.position = 0  # -1: Short, 0: Flat, 1: Long
        self.position_size = 0.0
        self.entry_price = 0.0
        self.entry_atr = 0.0  # Store ATR at entry for fixed SL/TP
        self.steps = 0
        self.total_pnl = 0.0
        self.trades = []
        self.prev_unrealized_pnl = 0.0  # Track for continuous PnL rewards
        # Holding-bonus state (reset on every new trade)
        self._holding_bonus_paid = 0.0
        self._holding_bonus_level = 0
        self.break_even_activated = False  # Reset break-even flag
        # High-water mark for progressive rewards (reset on every new trade)
        self._profit_high_water_mark = 0.0

        # Precompute context vectors if analyst is provided
        self._precomputed_contexts = None
        self._precomputed_probs = None
        
        # Use pre-computed cache if provided (for sequential context)
        self._precomputed_activations = {}
        if self.use_analyst and precomputed_analyst_cache is not None:
            logger.debug("Using pre-computed Analyst cache (sequential context)")
            self._precomputed_contexts = precomputed_analyst_cache['contexts'].astype(np.float32, copy=False)
            self._precomputed_probs = precomputed_analyst_cache['probs'].astype(np.float32, copy=False)

            # Load activations if available
            if 'activations_15m' in precomputed_analyst_cache and precomputed_analyst_cache['activations_15m'] is not None:
                self._precomputed_activations['15m'] = precomputed_analyst_cache['activations_15m'].astype(np.float32, copy=False)
            if 'activations_1h' in precomputed_analyst_cache and precomputed_analyst_cache['activations_1h'] is not None:
                self._precomputed_activations['1h'] = precomputed_analyst_cache['activations_1h'].astype(np.float32, copy=False)
            if 'activations_4h' in precomputed_analyst_cache and precomputed_analyst_cache['activations_4h'] is not None:
                self._precomputed_activations['4h'] = precomputed_analyst_cache['activations_4h'].astype(np.float32, copy=False)

            logger.debug(f"Loaded {len(self._precomputed_contexts)} cached context vectors")
        elif self.use_analyst and self.analyst is not None:
            self._precompute_contexts()
        # else: use_analyst=False - skip all analyst precomputation

    def _precompute_contexts(self):
        """Precompute all context vectors for efficiency."""
        if self.analyst is None:
            return

        logger.debug("Precomputing context vectors...")
        self.analyst.eval()

        contexts = []
        probs_list = []
        batch_size = 64

        with torch.no_grad():
            for i in range(0, self.n_samples, batch_size):
                end_i = min(i + batch_size, self.n_samples)
                actual_indices = range(self.start_idx + i, self.start_idx + end_i)

                # Get batch data (base/mid/high)
                batch_5m = torch.tensor(
                    self.data_5m[list(actual_indices)],
                    device=self.device,
                    dtype=torch.float32
                )
                batch_15m = torch.tensor(
                    self.data_15m[list(actual_indices)],
                    device=self.device,
                    dtype=torch.float32
                )
                batch_45m = torch.tensor(
                    self.data_45m[list(actual_indices)],
                    device=self.device,
                    dtype=torch.float32
                )

                # Get context AND probabilities
                if hasattr(self.analyst, 'get_probabilities'):
                    res = self.analyst.get_probabilities(batch_5m, batch_15m, batch_45m)
                    
                    if len(res) == 3:
                         context, probs, weights = res
                    else:
                         context, probs = res
                         weights = None
                         
                    contexts.append(context.cpu().numpy())
                    probs_list.append(probs.cpu().numpy())
                else:
                    # Fallback for old models
                    context = self.analyst.get_context(batch_5m, batch_15m, batch_45m)
                    contexts.append(context.cpu().numpy())
                    # Default probs (neutral)
                    dummy_probs = np.zeros((len(context), 3), dtype=np.float32)
                    dummy_probs[:, 1] = 1.0 # All neutral
                    probs_list.append(dummy_probs)

                # Memory cleanup
                del batch_5m, batch_15m, batch_45m, context
                if i % (batch_size * 10) == 0:
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()
                    gc.collect()

        self._precomputed_contexts = np.vstack(contexts).astype(np.float32)
        self._precomputed_probs = np.vstack(probs_list).astype(np.float32)

        logger.debug(f"Precomputed {len(self._precomputed_contexts)} context vectors and probabilities")

    def _get_analyst_data(self, idx: int) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray], Optional[Dict[str, np.ndarray]]]:
        """Get context vector, probabilities, attention weights, and activations for current index."""
        if self._precomputed_contexts is not None and self._precomputed_probs is not None:
            # Use precomputed
            context_idx = idx - self.start_idx
            if 0 <= context_idx < len(self._precomputed_contexts):
                # Get activations if available
                activations = None
                if self._precomputed_activations:
                    activations = {
                        k: v[context_idx] for k, v in self._precomputed_activations.items()
                    }

                return (
                    self._precomputed_contexts[context_idx],
                    self._precomputed_probs[context_idx],
                    None,
                    activations
                )

        if self.analyst is not None:
            # Compute on-the-fly
            with torch.no_grad():
                x_5m = torch.tensor(
                    self.data_5m[idx:idx+1],
                    device=self.device,
                    dtype=torch.float32
                )
                x_15m = torch.tensor(
                    self.data_15m[idx:idx+1],
                    device=self.device,
                    dtype=torch.float32
                )
                x_45m = torch.tensor(
                    self.data_45m[idx:idx+1],
                    device=self.device,
                    dtype=torch.float32
                )
                
                if hasattr(self.analyst, 'get_activations'):
                    context, activations = self.analyst.get_activations(x_5m, x_15m, x_45m)
                    
                    # Convert activations to numpy
                    activations_np = {
                        k: v.cpu().numpy().flatten() for k, v in activations.items()
                    }
                    
                    # Get probs
                    if hasattr(self.analyst, 'get_probabilities'):
                        res = self.analyst.get_probabilities(x_5m, x_15m, x_45m)

                        if isinstance(res, (tuple, list)) and len(res) == 3:
                            _, probs, _ = res
                        else:
                            _, probs = res
                        probs = probs.cpu().numpy().flatten()
                    else:
                        probs = np.array([0.5, 0.5], dtype=np.float32)
                        
                    return context.cpu().numpy().flatten(), probs, None, activations_np
                
                elif hasattr(self.analyst, 'get_probabilities'):
                    # Check if get_probabilities returns 3 values (new version)
                    result = self.analyst.get_probabilities(x_5m, x_15m, x_45m)
                    if len(result) == 3:
                        context, probs, weights = result
                        weights = weights.cpu().numpy().flatten() if weights is not None else None
                    else:
                        context, probs = result
                        weights = None
                    return context.cpu().numpy().flatten(), probs.cpu().numpy().flatten(), weights, None
                else:
                    context = self.analyst.get_context(x_5m, x_15m, x_45m)
                    # Dummy probs - match num_classes
                    if self.num_classes == 2:
                        probs = np.array([0.5, 0.5], dtype=np.float32)  # Binary: neutral
                    else:
                        probs = np.array([0.0, 1.0, 0.0], dtype=np.float32)  # Multi: neutral
                    return context.cpu().numpy().flatten(), probs, None, None

        # No analyst - return zeros with correct probs size
        if self.num_classes == 2:
            dummy_probs = np.array([0.5, 0.5], dtype=np.float32)  # Binary: neutral
        else:
            dummy_probs = np.array([0.0, 1.0, 0.0], dtype=np.float32)  # Multi: neutral
        return (
            np.zeros(self.context_dim, dtype=np.float32),
            dummy_probs,
            None,
            None
        )

    def _should_block_trade(self, direction: int) -> bool:
        """
        v37 Trade Filters: Check if trade should be blocked based on time.

        Blocks trades during:
        - Problematic hours (e.g., 6-7 PM UTC = low liquidity)
        - Problematic days (e.g., Sunday = 11.1% win rate in OOS analysis)

        Args:
            direction: Requested trade direction (0=Flat, 1=Long, 2=Short)

        Returns:
            True if trade should be blocked, False otherwise
        """
        # Only block entry trades, not exits
        if direction == 0:
            return False

        if not self.use_trade_filters:
            return False

        # Get current hour/day from timestamps
        if self.timestamps is not None and self.current_idx < len(self.timestamps):
            try:
                current_ts = self.timestamps[self.current_idx]
                # Convert Unix timestamp to datetime
                dt = pd.Timestamp(current_ts, unit='s', tz='UTC')
                hour = dt.hour
                day = dt.dayofweek  # 0=Monday, 6=Sunday

                if hour in self.blocked_hours:
                    return True
                if day in self.blocked_days:
                    return True
            except Exception:
                pass  # Silently ignore timestamp parsing errors

        return False

    def _get_observation(self) -> np.ndarray:
        """Construct observation vector."""
        # Context vector and probabilities (only when analyst is enabled)
        if self.use_analyst:
            context, probs, weights, activations = self._get_analyst_data(self.current_idx)
            self.current_probs = probs  # Store for action enforcement
            self.current_activations = activations  # Store for info

            # Calculate Analyst metrics for observation
            # [p_down, p_up, confidence, edge]
            if len(probs) == 2 or self.num_classes == 2:
                p_down = probs[0]
                p_up = probs[1] if len(probs) > 1 else 1 - p_down
                confidence = max(p_down, p_up)
                edge = p_up - p_down
                uncertainty = 1.0 - confidence
                analyst_metrics = np.array([p_down, p_up, edge, confidence, uncertainty], dtype=np.float32)
            else:
                # Multi-class: [p_down, p_neutral, p_up]
                p_down = probs[0]
                p_neutral = probs[1]
                p_up = probs[2]
                confidence = np.max(probs)  # Use np.max for multi-class confidence
                edge = p_up - p_down
                uncertainty = 1.0 - confidence
                analyst_metrics = np.array([p_down, p_neutral, p_up, edge, confidence, uncertainty], dtype=np.float32)
        else:
            # No analyst mode - empty context and metrics
            context = np.array([], dtype=np.float32)
            analyst_metrics = np.array([], dtype=np.float32)
            # Set dummy probs for compatibility (action enforcement disabled anyway)
            self.current_probs = np.array([0.5, 0.5], dtype=np.float32)
            self.current_activations = None

        # Position state
        current_price = self.close_prices[self.current_idx]
        atr = self.market_features[self.current_idx, 0] if len(self.market_features.shape) > 1 else 1.0

        # Normalize entry price and unrealized PnL
        # CRITICAL FIX: Use floor for ATR to prevent division by near-zero
        atr_safe = max(atr, 1e-6)
        if self.position != 0:
            # FIX: entry_price_norm should be POSITIVE when position is profitable
            # Previously was inverted for Long positions (winning Long = negative value)
            if self.position == 1:  # Long: positive when price goes UP (winning)
                entry_price_norm = (current_price - self.entry_price) / atr_safe
            else:  # Short: positive when price goes DOWN (winning)
                entry_price_norm = (self.entry_price - current_price) / atr_safe
            # Clip to prevent extreme values
            entry_price_norm = np.clip(entry_price_norm, -10.0, 10.0)
            # FIX: Divide by position_size so unrealized_pnl_norm is on same scale as entry_price_norm
            unrealized_pnl_raw = self._calculate_unrealized_pnl() / max(self.position_size, 0.01)
            unrealized_pnl_norm = unrealized_pnl_raw / atr_safe  # Normalize by ATR (consistent with entry_price_norm)
        else:
            entry_price_norm = 0.0
            unrealized_pnl_norm = 0.0

        # Add time in trade (normalized to [0, 1])
        # This helps agent understand how long it's been holding - prevents scalping
        if self.position != 0:
            bars_held = self.current_idx - self.entry_idx
            time_in_trade_norm = min(bars_held / 100.0, 1.0)  # Caps at 100 bars
        else:
            time_in_trade_norm = 0.0

        position_state = np.array([
            float(self.position),
            entry_price_norm,
            unrealized_pnl_norm,
            time_in_trade_norm  # NEW: How long held (0 = just entered, 1 = 100+ bars)
        ], dtype=np.float32)

        # Market features (NORMALIZED to prevent scale inconsistencies)
        if len(self.market_features.shape) > 1:
            market_feat_raw = self.market_features[self.current_idx]
            
            # Rolling window normalization for non-stationary financial data
            # OPTIMIZED: O(1) incremental updates using running sums
            has_enough_samples = self.rolling_count >= self.rolling_min_samples
            
            if self.use_rolling_norm and has_enough_samples:
                # O(1) UPDATE: Evict oldest value if buffer is full, add new value
                if self.rolling_count >= self.rolling_window_size:
                    # Evict oldest value from running sums
                    old_val = self.rolling_buffer[self.rolling_idx]
                    self.rolling_sum -= old_val
                    self.rolling_sum_sq -= old_val ** 2
                
                # Add new value
                self.rolling_buffer[self.rolling_idx] = market_feat_raw
                self.rolling_sum += market_feat_raw
                self.rolling_sum_sq += market_feat_raw ** 2
                
                # Update circular index
                self.rolling_idx = (self.rolling_idx + 1) % self.rolling_window_size
                if self.rolling_count < self.rolling_window_size:
                    self.rolling_count += 1
                
                # O(1) mean/std calculation from running sums
                n = self.rolling_count
                rolling_means = (self.rolling_sum / n).astype(np.float32)
                variance = (self.rolling_sum_sq / n) - (rolling_means ** 2)
                rolling_stds = np.maximum(np.sqrt(np.maximum(variance, 0)), 1e-6).astype(np.float32)
                
                # Apply rolling normalization
                market_feat = ((market_feat_raw - rolling_means) / rolling_stds).astype(np.float32)
                
                # Safety clip to ±5.0 to prevent extreme values
                market_feat = np.clip(market_feat, -self.clip_value, self.clip_value)
                
            # Fallback to global stats if rolling not ready
            elif self.market_feat_mean is not None and self.market_feat_std is not None:
                # Still update buffer so it fills up for rolling norm later
                if self.use_rolling_norm:
                    self.rolling_buffer[self.rolling_idx] = market_feat_raw
                    self.rolling_sum += market_feat_raw
                    self.rolling_sum_sq += market_feat_raw ** 2
                    self.rolling_idx = (self.rolling_idx + 1) % self.rolling_window_size
                    self.rolling_count += 1
                
                market_feat = ((market_feat_raw - self.market_feat_mean) /
                              self.market_feat_std).astype(np.float32)
                market_feat = np.clip(market_feat, -self.clip_value, self.clip_value)
            else:
                market_feat = market_feat_raw

            if self._market_skip_norm_idx.size:
                market_feat[self._market_skip_norm_idx] = market_feat_raw[self._market_skip_norm_idx]
        else:
            market_feat = np.zeros(5, dtype=np.float32)

        # Combine all features
        # v40 FIX: Updated comment to reflect actual observation layout:
        # [context (64), position (4), market (33*3=99), analyst_metrics (5-6), sl_tp (2), hold (4), returns (12)]
        
        # SL/TP Blind Spot Fix: Calculate normalized distance to expected SL/TP levels
        dist_sl_norm = 0.0
        dist_tp_norm = 0.0
        
        if self.position != 0 and len(self.market_features.shape) > 1:
            # PARITY FIX: Use entry ATR for SL/TP observation to match enforcement
            # (_check_stop_loss_take_profit uses self.entry_atr, not current ATR).
            current_atr = self.market_features[self.current_idx, 0]
            atr = self.entry_atr if self.entry_atr > 0 else current_atr
            if atr > 1e-8:
                sl_pips = max((atr * self.sl_atr_multiplier) / self.pip_value, 5.0)
                tp_pips = max((atr * self.tp_atr_multiplier) / self.pip_value, 5.0)
                pip_value = self.pip_value

                if self.position == 1: # Long
                    sl_price = self.entry_price - sl_pips * pip_value
                    tp_price = self.entry_price + tp_pips * pip_value
                    # PARITY FIX: Reflect break-even in SL observation
                    if self.break_even_activated:
                        sl_price = self.entry_price
                    dist_sl_norm = (current_price - sl_price) / atr
                    dist_tp_norm = (tp_price - current_price) / atr
                else: # Short
                    sl_price = self.entry_price + sl_pips * pip_value
                    tp_price = self.entry_price - tp_pips * pip_value
                    # PARITY FIX: Reflect break-even in SL observation
                    if self.break_even_activated:
                        sl_price = self.entry_price
                    dist_sl_norm = (sl_price - current_price) / atr
                    dist_tp_norm = (current_price - tp_price) / atr

        # Hold-Encouraging Features
        profit_progress = 0.0  # 0 = at entry, 1 = at TP
        dist_to_tp_pct = 1.0   # 1 = at entry, 0 = at TP  
        momentum_aligned = 0.0  # Positive = price moving in trade direction
        session_progress = 0.0  # 0-1 based on hour of day

        if self.position != 0 and len(self.market_features.shape) > 1:
            # PARITY FIX: Use entry ATR for TP target to match enforcement
            # (_check_stop_loss_take_profit uses self.entry_atr, not current ATR)
            atr = self.entry_atr if self.entry_atr > 0 else self.market_features[self.current_idx, 0]
            if atr > 1e-8:
                tp_target = self.tp_atr_multiplier * atr

                # Profit Progress: How far toward TP (0 = entry, 1 = at TP)
                # FIX: Divide by position_size so progress reflects raw price distance, not sized PnL
                unrealized = (self._calculate_unrealized_pnl() / max(self.position_size, 0.01)) * self.pip_value  # Convert to price units
                profit_progress = np.clip(unrealized / tp_target, -1.0, 1.0)
                
                # Distance to TP as percentage (1 = at entry, 0 = at TP)
                dist_to_tp_pct = np.clip(1.0 - profit_progress, 0.0, 2.0)
                
                # Momentum Aligned: Is recent price movement in trade direction?
                if self.current_idx >= 5:
                    price_5_bars_ago = self.close_prices[self.current_idx - 5]
                    price_change = (current_price - price_5_bars_ago) / atr
                    momentum_aligned = price_change * self.position  # Positive = good for trade
                    momentum_aligned = np.clip(momentum_aligned, -2.0, 2.0)

        # Session Progress: Normalized hour of day (0 = midnight, 0.5 = noon)
        # Use timestamps if available, else use step % 288 (5m bars in a day)
        if self.timestamps is not None and self.current_idx < len(self.timestamps):
            ts = self.timestamps[self.current_idx]
            hour = (ts % 86400) // 3600  # Seconds since midnight -> hour
            session_progress = hour / 24.0
        else:
            session_progress = (self.current_idx % 288) / 288.0

        # Hold features array
        # Add noise to momentum_aligned only (derived from price, benefits from regularization)
        if self.noise_level > 0:
            momentum_aligned_noisy = momentum_aligned + self.np_random.normal(0, self.noise_level)
        else:
            momentum_aligned_noisy = momentum_aligned
            
        hold_features = np.array([
            profit_progress,         # No noise - exact TP progress
            dist_to_tp_pct,          # No noise - exact distance
            momentum_aligned_noisy,  # With noise - regularized market signal
            session_progress         # No noise - exact time
        ], dtype=np.float32)

        # Apply noise ONLY to market features (not position state, SL/TP, hold features)
        # This preserves exact state information while regularizing market signals
        if self.noise_level > 0:
            market_feat_noisy = market_feat + self.np_random.normal(0, self.noise_level, size=market_feat.shape).astype(np.float32)
        else:
            market_feat_noisy = market_feat

        obs = np.concatenate([
            context,
            position_state,           # No noise - exact position info
            market_feat_noisy,        # With noise - regularized market signals
            analyst_metrics,
            np.array([dist_sl_norm, dist_tp_norm], dtype=np.float32),  # No noise - exact SL/TP
            hold_features,            # No noise - exact hold state
        ])
        
        # Append "Full Eyes" features (no noise - exact returns)
        if self.agent_lookback_window > 0 and self.returns is not None:
            # Slice recent returns
            # Use current_idx + 1 because the 'returns' array aligns with close_prices
            # We want [t - lookback + 1 ... t]
            idx_start = self.current_idx - self.agent_lookback_window + 1
            idx_end = self.current_idx + 1
            if idx_start < 0:
                # Pad with zeros if we are at the very beginning (unlikely due to start_idx)
                returns_slice = np.zeros(self.agent_lookback_window, dtype=np.float32)
                valid_len = idx_end
                returns_slice[-valid_len:] = self.returns[0:idx_end]
            else:
                returns_slice = self.returns[idx_start:idx_end]
            
            # v26 FIX: Returns are already normalized in parquet, no scaling needed
            # Previous * 100 created [-500, 500] range, destroying PPO stability
            obs = np.concatenate([obs, returns_slice])

        return obs.astype(np.float32)

    def _calculate_unrealized_pnl(self) -> float:
        """Calculate unrealized PnL in pips."""
        if self.position == 0:
            return 0.0

        current_price = self.close_prices[self.current_idx]
        pip_value = self.pip_value  # US30: 1.0 per point

        if self.position == 1:  # Long
            pnl_pips = (current_price - self.entry_price) / pip_value
        else:  # Short
            pnl_pips = (self.entry_price - current_price) / pip_value

        return pnl_pips * self.position_size

    def _check_stop_loss_take_profit(self) -> Tuple[float, dict]:
        """
        Check and execute stop-loss or take-profit if triggered.

        FIXED: Now uses High/Low to detect intra-bar SL/TP hits, not just Close.
        This creates more realistic training by penalizing the agent for positions
        that would have been stopped out by intra-bar wicks in real trading.

        This method is called BEFORE the agent's action to enforce risk management.
        Stop-loss cuts losing positions early to prevent catastrophic losses.
        Take-profit locks in gains to improve risk/reward ratio.

        Returns:
            Tuple of (reward, info_dict) if triggered, (0.0, {}) otherwise
        """
        # No position = nothing to check
        if self.position == 0:
            return 0.0, {}

        # Get current bar OHLC
        close_price = self.close_prices[self.current_idx]
        pip_value = self.pip_value  # US30: 1.0 per point

        # FIXED: Get High/Low for accurate intra-bar SL/TP detection
        if self.ohlc_data is not None:
            # ohlc_data shape: (n_samples, 4) = [open, high, low, close]
            high_price = float(self.ohlc_data[self.current_idx, 1])
            low_price = float(self.ohlc_data[self.current_idx, 2])
        else:
            # Fallback: use close price for all (legacy behavior)
            high_price = close_price
            low_price = close_price

        # v27 FIX: Use ATR stored at entry for FIXED SL/TP levels
        # This ensures risk is known at entry and doesn't widen during volatility spikes
        atr = self.entry_atr if self.entry_atr > 0 else (self.market_features[self.current_idx, 0] if len(self.market_features.shape) > 1 else 0.001)

        # Calculate dynamic thresholds in pips/points
        sl_pips_threshold = (atr * self.sl_atr_multiplier) / self.pip_value
        tp_pips_threshold = (atr * self.tp_atr_multiplier) / self.pip_value

        # Enforce minimums (e.g. 5 pips) to prevent noise stop-outs
        sl_pips_threshold = max(sl_pips_threshold, 5.0)
        tp_pips_threshold = max(tp_pips_threshold, 5.0)

        # BREAK-EVEN STOP LOSS
        # If profit reaches break_even_atr * ATR, move SL to entry price
        # Once activated, SL stays at entry even if price retraces
        break_even_profit_pips = (atr * self.break_even_atr) / self.pip_value
        # v26 FIX: Divide by position_size so comparison is pips vs pips (not pip-lots vs pips)
        current_unrealized_pips = self._calculate_unrealized_pnl() / max(self.position_size, 0.01)

        if self.break_even_atr > 0 and not self.break_even_activated and current_unrealized_pips >= break_even_profit_pips:
            self.break_even_activated = True  # Lock in break-even

        if self.break_even_activated:
            sl_pips_threshold = 0.0  # SL at entry price

        # Calculate SL/TP price levels
        if self.position == 1:  # Long
            sl_price = self.entry_price - sl_pips_threshold * pip_value
            tp_price = self.entry_price + tp_pips_threshold * pip_value

            # For Long: SL triggered if Low <= SL price, TP triggered if High >= TP price
            # IMPORTANT: Check SL first (worst case) to be conservative
            if self.use_stop_loss and low_price <= sl_price:
                # Exit at SL level (not at the low - we don't know exact fill)
                exit_price = sl_price
                pnl_pips = -sl_pips_threshold * self.position_size
                # FIX: Keep PnL in PIPS (same unit as _calculate_unrealized_pnl)
                # Previously multiplied by 10 (dollars) which broke reward calculation
                pnl = pnl_pips

                self.total_pnl += pnl
                self.trades.append({
                    'entry': self.entry_price,
                    'exit': exit_price,
                    'direction': self.position,
                    'size': self.position_size,
                    'pnl': pnl,

                    'bars_held': self.current_idx - self.entry_idx,
                    'close_reason': 'stop_loss'
                })

                # Calculate final delta before reset (both in PIPS now)
                final_delta = pnl - self.prev_unrealized_pnl

                # v31 FIX: Save position info for alpha calculation before reset
                saved_position = self.position  # 1 = Long
                saved_position_size = self.position_size

                # Reset
                self.position = 0
                self.position_size = 0.0
                self.entry_price = 0.0
                self.prev_unrealized_pnl = 0.0
                self._holding_bonus_paid = 0.0
                self._holding_bonus_level = 0
                self._profit_high_water_mark = 0.0
                self.break_even_activated = False
                self.entry_atr = 0.0

                # v24 FIX: In sparse mode, reward FULL trade PnL
                if self.use_sparse_rewards:
                    sl_reward = pnl * self.reward_scaling
                else:
                    # v31 FIX: Apply alpha to SL reward (Long closing)
                    if self.use_alpha_reward and self.current_idx > 0:
                        current_price = self.close_prices[self.current_idx]
                        prev_price = self.close_prices[self.current_idx - 1]
                        market_move_pips = (current_price - prev_price) / self.pip_value
                        # v41 FIX: Directional baseline — closing LONG, so buy-and-hold baseline
                        exit_baseline = market_move_pips * self.alpha_baseline_exposure * saved_position_size
                        exit_alpha = final_delta - exit_baseline
                        if exit_alpha > 0:
                            sl_reward = exit_alpha * self.profit_scaling
                        else:
                            sl_reward = exit_alpha * self.loss_scaling
                    else:
                        sl_reward = final_delta * self.reward_scaling

                return sl_reward, {
                    'stop_loss_triggered': True,
                    'trade_closed': True,
                    'close_reason': 'stop_loss',
                    'pnl': pnl
                }

            if self.use_take_profit and high_price >= tp_price:
                # Exit at TP level
                exit_price = tp_price
                pnl_pips = tp_pips_threshold * self.position_size
                # FIX: Keep PnL in PIPS (same unit as _calculate_unrealized_pnl)
                pnl = pnl_pips

                self.total_pnl += pnl
                self.trades.append({
                    'entry': self.entry_price,
                    'exit': exit_price,
                    'direction': self.position,
                    'size': self.position_size,
                    'pnl': pnl,

                    'bars_held': self.current_idx - self.entry_idx,
                    'close_reason': 'take_profit'
                })

                # Calculate final delta before reset (both in PIPS now)
                final_delta = pnl - self.prev_unrealized_pnl

                # v31 FIX: Save position info for alpha calculation before reset
                saved_position = self.position  # 1 = Long
                saved_position_size = self.position_size

                self.position = 0
                self.position_size = 0.0
                self.entry_price = 0.0
                self.prev_unrealized_pnl = 0.0
                self._holding_bonus_paid = 0.0
                self._holding_bonus_level = 0
                self._profit_high_water_mark = 0.0
                self.break_even_activated = False
                self.entry_atr = 0.0

                # v24 FIX: In sparse mode, reward FULL trade PnL
                if self.use_sparse_rewards:
                    tp_reward = pnl * self.reward_scaling
                else:
                    # v31 FIX: Apply alpha to TP reward (Long closing)
                    if self.use_alpha_reward and self.current_idx > 0:
                        current_price = self.close_prices[self.current_idx]
                        prev_price = self.close_prices[self.current_idx - 1]
                        market_move_pips = (current_price - prev_price) / self.pip_value
                        # v41 FIX: Directional baseline — closing LONG, so buy-and-hold baseline
                        exit_baseline = market_move_pips * self.alpha_baseline_exposure * saved_position_size
                        exit_alpha = final_delta - exit_baseline
                        if exit_alpha > 0:
                            tp_reward = exit_alpha * self.profit_scaling
                        else:
                            tp_reward = exit_alpha * self.loss_scaling
                    else:
                        tp_reward = final_delta * self.reward_scaling

                return tp_reward, {
                    'take_profit_triggered': True,
                    'trade_closed': True,
                    'close_reason': 'take_profit',
                    'pnl': pnl
                }

        else:  # Short (position == -1)
            sl_price = self.entry_price + sl_pips_threshold * pip_value
            tp_price = self.entry_price - tp_pips_threshold * pip_value

            # For Short: SL triggered if High >= SL price, TP triggered if Low <= TP price
            if self.use_stop_loss and high_price >= sl_price:
                # Exit at SL level
                exit_price = sl_price
                pnl_pips = -sl_pips_threshold * self.position_size
                # FIX: Keep PnL in PIPS (same unit as _calculate_unrealized_pnl)
                pnl = pnl_pips

                self.total_pnl += pnl
                self.trades.append({
                    'entry': self.entry_price,
                    'exit': exit_price,
                    'direction': self.position,
                    'size': self.position_size,
                    'pnl': pnl,

                    'bars_held': self.current_idx - self.entry_idx,
                    'close_reason': 'stop_loss'
                })

                # Calculate final delta before reset (both in PIPS now)
                final_delta = pnl - self.prev_unrealized_pnl

                # v31 FIX: Save position info for alpha calculation before reset
                saved_position = self.position  # -1 = Short
                saved_position_size = self.position_size

                self.position = 0
                self.position_size = 0.0
                self.entry_price = 0.0
                self.prev_unrealized_pnl = 0.0
                self._holding_bonus_paid = 0.0
                self._holding_bonus_level = 0
                self._profit_high_water_mark = 0.0
                self.break_even_activated = False
                self.entry_atr = 0.0

                # v24 FIX: In sparse mode, reward FULL trade PnL
                if self.use_sparse_rewards:
                    sl_reward = pnl * self.reward_scaling
                else:
                    # v31 FIX: Apply alpha to SL reward (Short closing)
                    if self.use_alpha_reward and self.current_idx > 0:
                        current_price = self.close_prices[self.current_idx]
                        prev_price = self.close_prices[self.current_idx - 1]
                        market_move_pips = (current_price - prev_price) / self.pip_value
                        # v41 FIX: Directional baseline — closing SHORT, so sell-and-hold baseline
                        exit_baseline = -market_move_pips * self.alpha_baseline_exposure * saved_position_size
                        exit_alpha = final_delta - exit_baseline
                        # Direction-compensated rewards for short positions
                        if exit_alpha > 0:
                            sl_reward = exit_alpha * self.profit_scaling * self.short_profit_multiplier
                        else:
                            sl_reward = exit_alpha * self.loss_scaling * self.short_loss_multiplier
                    else:
                        sl_reward = final_delta * self.reward_scaling

                return sl_reward, {
                    'stop_loss_triggered': True,
                    'trade_closed': True,
                    'close_reason': 'stop_loss',
                    'pnl': pnl
                }

            if self.use_take_profit and low_price <= tp_price:
                # Exit at TP level
                exit_price = tp_price
                pnl_pips = tp_pips_threshold * self.position_size
                # FIX: Keep PnL in PIPS (same unit as _calculate_unrealized_pnl)
                pnl = pnl_pips

                self.total_pnl += pnl
                self.trades.append({
                    'entry': self.entry_price,
                    'exit': exit_price,
                    'direction': self.position,
                    'size': self.position_size,
                    'pnl': pnl,

                    'bars_held': self.current_idx - self.entry_idx,
                    'close_reason': 'take_profit'
                })

                # Calculate final delta before reset (both in PIPS now)
                final_delta = pnl - self.prev_unrealized_pnl

                # v31 FIX: Save position info for alpha calculation before reset
                saved_position = self.position  # -1 = Short
                saved_position_size = self.position_size

                self.position = 0
                self.position_size = 0.0
                self.entry_price = 0.0
                self.prev_unrealized_pnl = 0.0
                self._holding_bonus_paid = 0.0
                self._holding_bonus_level = 0
                self._profit_high_water_mark = 0.0
                self.break_even_activated = False
                self.entry_atr = 0.0

                # v24 FIX: In sparse mode, reward FULL trade PnL
                if self.use_sparse_rewards:
                    tp_reward = pnl * self.reward_scaling
                else:
                    # v31 FIX: Apply alpha to TP reward (Short closing)
                    if self.use_alpha_reward and self.current_idx > 0:
                        current_price = self.close_prices[self.current_idx]
                        prev_price = self.close_prices[self.current_idx - 1]
                        market_move_pips = (current_price - prev_price) / self.pip_value
                        # v41 FIX: Directional baseline — closing SHORT, so sell-and-hold baseline
                        exit_baseline = -market_move_pips * self.alpha_baseline_exposure * saved_position_size
                        exit_alpha = final_delta - exit_baseline
                        # Direction-compensated rewards for short positions
                        if exit_alpha > 0:
                            tp_reward = exit_alpha * self.profit_scaling * self.short_profit_multiplier
                        else:
                            tp_reward = exit_alpha * self.loss_scaling * self.short_loss_multiplier
                    else:
                        tp_reward = final_delta * self.reward_scaling

                return tp_reward, {
                    'take_profit_triggered': True,
                    'trade_closed': True,
                    'close_reason': 'take_profit',
                    'pnl': pnl
                }

        return 0.0, {}

    def _execute_action(self, action: np.ndarray) -> Tuple[float, dict]:
        """
        Execute trading action and calculate reward.

        Returns:
            Tuple of (reward, info_dict)
        """
        direction = int(action[0])  # 0=Flat, 1=Long, 2=Short
        size_idx = int(action[1])   # 0-3

        # v37 Trade Filters: Block trades during problematic times
        if self._should_block_trade(direction):
            # Force to Flat/Exit during blocked times
            direction = 0

        # Enforce Analyst Alignment (Action Masking)
        if self.enforce_analyst_alignment and self.current_probs is not None:
            # Determine Analyst Direction
            # Binary: [p_down, p_up] -> 0=Down, 1=Up
            # Multi: [p_down, p_neutral, p_up] -> 0=Down, 1=Neutral, 2=Up
            
            analyst_dir = 0 # Default Flat
            
            if len(self.current_probs) == 2:
                # Binary: 0=Short, 1=Long (mapped to env: 2=Short, 1=Long)
                p_down, p_up = self.current_probs
                if p_up > 0.5:
                    analyst_dir = 1 # Long
                elif p_down > 0.5:
                    analyst_dir = 2 # Short
                # Else neutral/uncertain
                
            elif len(self.current_probs) == 3:
                # Multi: 0=Down, 1=Neutral, 2=Up
                p_down, p_neutral, p_up = self.current_probs
                max_idx = np.argmax(self.current_probs)
                if max_idx == 2: # Up
                    analyst_dir = 1 # Long
                elif max_idx == 0: # Down
                    analyst_dir = 2 # Short
                else:
                    analyst_dir = 0 # Flat
            
            # Check for violation
            # If Analyst is Long (1), Agent cannot be Short (2)
            # If Analyst is Short (2), Agent cannot be Long (1)
            # If Analyst is Flat (0), Agent must be Flat (0)
            
            violation = False
            if analyst_dir == 1 and direction == 2: # Analyst Long, Agent Short
                violation = True
            elif analyst_dir == 2 and direction == 1: # Analyst Short, Agent Long
                violation = True
            elif analyst_dir == 0 and direction != 0: # Analyst Flat, Agent Active
                violation = True
                
            if violation:
                # Force Flat Action
                direction = 0
                # Optional: Add small penalty? No, just prevent the action.
                # The agent will learn that this action does nothing.
        
        base_size = self.POSITION_SIZES[size_idx]
        
        # Dollar-Based Volatility Sizing: Maintain constant dollar risk per trade
        # Size = Risk($) / ($/pip × SL_pips)
        # This ensures each trade risks the same dollar amount regardless of volatility
        new_size = base_size
        
        if self.volatility_sizing and len(self.market_features.shape) > 1:
            atr = self.market_features[self.current_idx, 0]
            # Calculate SL distance in pips/points
            sl_pips = (atr * self.sl_atr_multiplier) / self.pip_value
            sl_pips = max(sl_pips, 5.0)  # Minimum 5 points
            
            # US30 dollar risk sizing:
            # Risk($) = size(lots) × sl_pips(points) × $/point
            # $/point per 1 lot = pip_value × lot_size × point_multiplier
            dollars_per_pip = self.pip_value * self.lot_size * self.point_multiplier
            risk_amount = self.risk_per_trade * base_size  # Scale risk by agent's size choice
            new_size = risk_amount / (dollars_per_pip * sl_pips)
            
            # Clip to reasonable limits to prevent extreme leverage
            new_size = np.clip(new_size, 0.1, 50.0)  # Max 50 lots

        # v37 OOD FIX: Adaptive position sizing based on training-anchored ood_score
        # More aggressive reduction than v36 because ood_score uses FIXED training stats
        # ood_score=0: full size, ood_score=1: min_position_size_ratio (e.g., 20%)
        if len(self.market_features.shape) > 1:
            try:
                from src.live.bridge_constants import MARKET_FEATURE_COLS
                n_features_per_tf = len(MARKET_FEATURE_COLS)
                n_total_features = self.market_features.shape[1]

                # Find ood_score index (last feature in each TF block)
                if 'ood_score' in MARKET_FEATURE_COLS:
                    ood_score_offset = MARKET_FEATURE_COLS.index('ood_score')
                    ood_score_idx = ood_score_offset  # First TF block (5m)

                    if ood_score_idx < n_total_features:
                        ood_score = self.market_features[self.current_idx, ood_score_idx]
                        # v37 OOD sizing: more aggressive reduction
                        # ood_score=0 -> multiplier=1.0, ood_score=1 -> multiplier=min_ratio
                        ood_multiplier = max(
                            self.min_position_size_ratio,
                            1.0 - self.ood_size_reduction_factor * ood_score
                        )
                        new_size *= ood_multiplier
                else:
                    pass  # No OOD score available; use full position size
            except Exception:
                pass  # Silently ignore if feature lookup fails

        pnl_delta = 0.0
        reward = 0.0
        info = {
            'trade_opened': False,
            'trade_closed': False,
            'pnl': 0.0,
            'executed_direction': direction  # v23.1: Track actual direction after analyst masking for backtest parity
        }

        current_price = self.close_prices[self.current_idx]
        prev_price = self.close_prices[self.current_idx - 1] if self.current_idx > 0 else current_price

        # Get market conditions
        if len(self.market_features.shape) > 1:
            atr = self.market_features[self.current_idx, 0]
            chop = self.market_features[self.current_idx, 1]
        else:
            atr = 0.001
            chop = 50.0

        # Calculate price move for FOMO detection
        price_move = abs(current_price - prev_price)
        pip_value = self.pip_value  # US30: 1.0 per point

        # Handle position changes
        # Reward structure: Pure continuous PnL delta rewards every step.
        # On close/flip: capture the final delta BEFORE resetting position so the last
        # price leg is not missed, without paying any extra "banked" exit reward.
        
        # MINIMUM HOLD TIME CHECK
        # Block manual exits AND position flips before min_hold_bars have passed
        # This prevents scalping by forcing agent to hold trades longer
        # SL/TP are NOT affected - they are checked BEFORE this action handling
        # PROFIT-BASED EARLY EXIT OVERRIDE
        # If profit exceeds early_exit_profit_atr * ATR, allow early exit
        if self.position != 0 and self.min_hold_bars > 0:
            bars_held = self.current_idx - self.entry_idx
            if bars_held < self.min_hold_bars:
                # Check if action would close or flip the position
                would_close_or_flip = (
                    direction == 0 or  # Flat/Exit
                    (self.position == 1 and direction == 2) or  # Long→Short flip
                    (self.position == -1 and direction == 1)    # Short→Long flip
                )
                if would_close_or_flip:
                    # Check for profit-based early exit override
                    allow_early_exit = False
                    if self.early_exit_profit_atr > 0:
                        # Get current ATR
                        if len(self.market_features.shape) > 1:
                            current_atr = self.market_features[self.current_idx, 0]
                        else:
                            current_atr = 20.0  # Fallback for US30
                        
                        # Calculate unrealized profit in pips (divide by position_size!)
                        # v26 FIX: _calculate_unrealized_pnl returns pips×lots, divide to get raw pips
                        unrealized_pnl = self._calculate_unrealized_pnl() / max(self.position_size, 0.01)
                        profit_threshold = self.early_exit_profit_atr * current_atr / self.pip_value
                        
                        if unrealized_pnl > profit_threshold:
                            allow_early_exit = True
                            info['early_exit_profit'] = True
                            info['profit_pips'] = unrealized_pnl
                            info['profit_threshold'] = profit_threshold
                    
                    if not allow_early_exit:
                        # BLOCK: Force agent to keep current position
                        direction = 1 if self.position == 1 else 2  # Keep Long/Short
                        info['exit_blocked'] = True
                        info['bars_held'] = bars_held
                        info['min_hold_bars'] = self.min_hold_bars
        
        if direction == 0:  # Flat/Exit
            if self.position != 0:
                # CRITICAL: Calculate final delta BEFORE resetting position
                # This captures the last price leg that would otherwise be missed
                final_unrealized = self._calculate_unrealized_pnl()
                final_delta = final_unrealized - self.prev_unrealized_pnl
                pnl_delta += final_delta
                
                # v24 FIX: In sparse mode, reward FULL trade PnL (not just final delta)
                # because no per-bar rewards were given during the trade
                if self.use_sparse_rewards:
                    reward += final_unrealized * self.reward_scaling
                else:
                    # v31 FIX: Apply alpha calculation to exit reward (same as holding)
                    # Without this, exit gives raw delta reward, bypassing alpha logic
                    if self.use_alpha_reward and self.current_idx > 0:
                        current_price = self.close_prices[self.current_idx]
                        prev_price = self.close_prices[self.current_idx - 1]
                        market_move_pips = (current_price - prev_price) / self.pip_value

                        # v41 FIX: Directional baseline (long→B&H, short→S&H)
                        if self.position == 1:
                            exit_baseline = market_move_pips * self.alpha_baseline_exposure * self.position_size
                        else:
                            exit_baseline = -market_move_pips * self.alpha_baseline_exposure * self.position_size

                        exit_alpha = final_delta - exit_baseline

                        # Direction-compensated rewards
                        if self.position == -1:  # Short position closing
                            if exit_alpha > 0:
                                reward += exit_alpha * self.profit_scaling * self.short_profit_multiplier
                            else:
                                reward += exit_alpha * self.loss_scaling * self.short_loss_multiplier
                        else:  # Long position closing
                            if exit_alpha > 0:
                                reward += exit_alpha * self.profit_scaling
                            else:
                                reward += exit_alpha * self.loss_scaling
                        info['exit_alpha'] = exit_alpha
                    else:
                        # Fallback: raw delta (when alpha disabled)
                        reward += final_delta * self.reward_scaling

                # REMOVED: Direction bonus was causing reward-PnL divergence
                # The bonus (+2.5 for ANY profitable trade) was 50x larger than
                # the PnL reward for tiny winners, teaching agent to make many
                # small trades to collect bonuses regardless of actual profitability.
                # PnL delta (above) is now the ONLY source of reward for exits.

                # Early Exit Penalty - penalize premature closes to discourage scalping
                bars_held = self.current_idx - self.entry_idx
                if bars_held < self.min_bars_before_exit and self.early_exit_penalty != 0:
                    reward += self.early_exit_penalty
                    info['early_exit_penalty'] = self.early_exit_penalty
                    info['bars_held_at_exit'] = bars_held

                # Record trade statistics
                info['trade_closed'] = True
                info['pnl'] = final_unrealized  # Unscaled for tracking
                info['pnl_delta'] = final_delta
                self.total_pnl += final_unrealized
                self.trades.append({
                    'entry': self.entry_price,
                    'exit': current_price,
                    'direction': self.position,
                    'size': self.position_size,
                    'pnl': final_unrealized,

                    'bars_held': self.current_idx - self.entry_idx
                })

                # NOW reset position state
                self.position = 0
                self.position_size = 0.0
                self.entry_price = 0.0
                self.prev_unrealized_pnl = 0.0
                self._holding_bonus_paid = 0.0
                self._holding_bonus_level = 0
                self._profit_high_water_mark = 0.0
                self.break_even_activated = False
                self.entry_atr = 0.0

        elif direction == 1:  # Long
            if self.position == -1:  # Close short first
                # CRITICAL: Calculate final delta BEFORE resetting position
                final_unrealized = self._calculate_unrealized_pnl()
                final_delta = final_unrealized - self.prev_unrealized_pnl
                pnl_delta += final_delta

                # v24 FIX: In sparse mode, reward FULL trade PnL
                if self.use_sparse_rewards:
                    reward += final_unrealized * self.reward_scaling
                else:
                    # v31 FIX: Apply alpha calculation to flip exit (Short→Long)
                    if self.use_alpha_reward and self.current_idx > 0:
                        current_price = self.close_prices[self.current_idx]
                        prev_price = self.close_prices[self.current_idx - 1]
                        market_move_pips = (current_price - prev_price) / self.pip_value
                        # v41 FIX: Directional baseline — closing SHORT, so sell-and-hold baseline
                        exit_baseline = -market_move_pips * self.alpha_baseline_exposure * self.position_size
                        exit_alpha = final_delta - exit_baseline
                        # Direction-compensated rewards for short position closing
                        if exit_alpha > 0:
                            reward += exit_alpha * self.profit_scaling * self.short_profit_multiplier
                        else:
                            reward += exit_alpha * self.loss_scaling * self.short_loss_multiplier
                        info['exit_alpha'] = exit_alpha
                    else:
                        reward += final_delta * self.reward_scaling

                # REMOVED: Direction bonus (see comment in Flat/Exit case above)

                # Early Exit Penalty - penalize premature flips
                bars_held = self.current_idx - self.entry_idx
                if bars_held < self.min_bars_before_exit and self.early_exit_penalty != 0:
                    reward += self.early_exit_penalty
                    info['early_exit_penalty'] = self.early_exit_penalty

                info['trade_closed'] = True
                info['pnl'] = final_unrealized
                info['pnl_delta'] = final_delta
                self.total_pnl += final_unrealized
                self.trades.append({
                    'entry': self.entry_price,
                    'exit': current_price,
                    'direction': -1,
                    'size': self.position_size,
                    'pnl': final_unrealized,

                    'bars_held': self.current_idx - self.entry_idx
                })

                # NOW reset position state before opening new one
                self.position = 0
                self.position_size = 0.0
                self.entry_price = 0.0
                self.prev_unrealized_pnl = 0.0
                self._holding_bonus_paid = 0.0
                self._holding_bonus_level = 0
                self._profit_high_water_mark = 0.0
                self.break_even_activated = False
                self.entry_atr = 0.0

            if self.position != 1:  # Open long
                self.position = 1
                self.position_size = new_size
                self.entry_price = current_price
                self.entry_atr = atr  # Store ATR at entry for fixed SL/TP
                self.entry_idx = self.current_idx  # Track entry bar for min hold time
                # Reset holding-bonus state for the new trade
                self._holding_bonus_paid = 0.0
                self._holding_bonus_level = 0
                # Total execution cost = spread + slippage (realistic modeling)
                exec_cost = (self.spread_pips + self.slippage_pips) * new_size
                reward -= exec_cost * self.reward_scaling
                # Include execution cost in total_pnl to match backtest accounting
                self.total_pnl -= exec_cost
                info['trade_opened'] = True

                # v15 FIX: Trade entry bonus to encourage exploration
                # Without this, every trade starts with negative reward (entry cost)
                # which teaches the agent that trading is bad before it can learn profitability
                reward += self.trade_entry_bonus
                info['trade_entry_bonus'] = self.trade_entry_bonus

        elif direction == 2:  # Short
            if self.position == 1:  # Close long first
                # CRITICAL: Calculate final delta BEFORE resetting position
                final_unrealized = self._calculate_unrealized_pnl()
                final_delta = final_unrealized - self.prev_unrealized_pnl
                pnl_delta += final_delta

                # v24 FIX: In sparse mode, reward FULL trade PnL
                if self.use_sparse_rewards:
                    reward += final_unrealized * self.reward_scaling
                else:
                    # v31 FIX: Apply alpha calculation to flip exit (Long→Short)
                    if self.use_alpha_reward and self.current_idx > 0:
                        current_price = self.close_prices[self.current_idx]
                        prev_price = self.close_prices[self.current_idx - 1]
                        market_move_pips = (current_price - prev_price) / self.pip_value
                        # v41 FIX: Directional baseline — closing LONG, so buy-and-hold baseline
                        exit_baseline = market_move_pips * self.alpha_baseline_exposure * self.position_size
                        exit_alpha = final_delta - exit_baseline
                        if exit_alpha > 0:
                            reward += exit_alpha * self.profit_scaling
                        else:
                            reward += exit_alpha * self.loss_scaling
                        info['exit_alpha'] = exit_alpha
                    else:
                        reward += final_delta * self.reward_scaling

                # REMOVED: Direction bonus (see comment in Flat/Exit case above)

                # Early Exit Penalty - penalize premature flips
                bars_held = self.current_idx - self.entry_idx
                if bars_held < self.min_bars_before_exit and self.early_exit_penalty != 0:
                    reward += self.early_exit_penalty
                    info['early_exit_penalty'] = self.early_exit_penalty

                info['trade_closed'] = True
                info['pnl'] = final_unrealized
                info['pnl_delta'] = final_delta
                self.total_pnl += final_unrealized
                self.trades.append({
                    'entry': self.entry_price,
                    'exit': current_price,
                    'direction': 1,
                    'size': self.position_size,
                    'pnl': final_unrealized,

                    'bars_held': self.current_idx - self.entry_idx
                })

                # NOW reset position state before opening new one
                self.position = 0
                self.position_size = 0.0
                self.entry_price = 0.0
                self.prev_unrealized_pnl = 0.0
                self._holding_bonus_paid = 0.0
                self._holding_bonus_level = 0
                self._profit_high_water_mark = 0.0
                self.break_even_activated = False
                self.entry_atr = 0.0

            if self.position != -1:  # Open short
                self.position = -1
                self.position_size = new_size
                self.entry_price = current_price
                self.entry_atr = atr  # Store ATR at entry for fixed SL/TP
                self.entry_idx = self.current_idx  # Track entry bar for min hold time
                # Reset holding-bonus state for the new trade
                self._holding_bonus_paid = 0.0
                self._holding_bonus_level = 0
                # Total execution cost = spread + slippage (realistic modeling)
                exec_cost = (self.spread_pips + self.slippage_pips) * new_size
                reward -= exec_cost * self.reward_scaling
                # Include execution cost in total_pnl to match backtest accounting
                self.total_pnl -= exec_cost
                info['trade_opened'] = True

                # v15 FIX: Trade entry bonus to encourage exploration
                # Without this, every trade starts with negative reward (entry cost)
                # which teaches the agent that trading is bad before it can learn profitability
                reward += self.trade_entry_bonus
                info['trade_entry_bonus'] = self.trade_entry_bonus

        # SIMPLIFIED CONTINUOUS PNL REWARD
        # Pure mark-to-market PnL delta - the agent receives reward proportional to
        # the actual change in unrealized PnL each step. This is the correct signal
        # for learning trading behavior:
        #   - Price moves in favor → positive reward
        #   - Price moves against → negative reward
        #   - Agent learns "price up = good for long, price down = bad for long"
        #
        # v24 FIX: Only apply continuous rewards if sparse mode is DISABLED.
        # When use_sparse_rewards=True, agent only gets reward on trade exit.
        if self.position != 0:
            current_unrealized_pnl = self._calculate_unrealized_pnl()
            pnl_delta = current_unrealized_pnl - self.prev_unrealized_pnl

            # Only add per-bar PnL if NOT using sparse rewards
            if not self.use_sparse_rewards:
                # Alpha-Based Reward - rewards OUTPERFORMANCE vs market, not absolute PnL
                # This removes directional bias: being long in bull market = 0 alpha
                if self.use_alpha_reward and self.current_idx > 0:
                    # Calculate market move (what buy-and-hold would have made)
                    current_price = self.close_prices[self.current_idx]
                    prev_price = self.close_prices[self.current_idx - 1]
                    market_move_pips = (current_price - prev_price) / self.pip_value

                    # v41 FIX: Directional alpha baseline
                    # Each direction is compared to its own passive strategy:
                    #   Long  → compared to buy-and-hold  (baseline = +market_move)
                    #   Short → compared to sell-and-hold (baseline = -market_move)
                    # This gives alpha=0 when matching the passive version of your direction
                    if self.position == 1:  # Long
                        baseline_pnl = market_move_pips * self.alpha_baseline_exposure * self.position_size
                    elif self.position == -1:  # Short
                        baseline_pnl = -market_move_pips * self.alpha_baseline_exposure * self.position_size
                    else:
                        baseline_pnl = 0

                    # Alpha = actual PnL - baseline PnL (outperformance vs passive strategy)
                    # Long +100 when market +100: alpha = 100 - 30 = +70 (good, followed trend)
                    # Short +100 when market -100: alpha = 100 - 30 = +70 (good, followed trend)
                    # Long -100 when market -100: alpha = -100 - (-30) = -70 (bad, fought trend)
                    # Short -100 when market +100: alpha = -100 - (-30) = -70 (bad, fought trend)
                    alpha = pnl_delta - baseline_pnl

                    # Apply direction-compensated scaling to alpha
                    # Short positions get boosted profits and reduced loss penalties
                    # to compensate for the bullish bias in training data
                    if self.position == -1:  # Short position
                        if alpha > 0:
                            reward += alpha * self.profit_scaling * self.short_profit_multiplier
                        else:
                            reward += alpha * self.loss_scaling * self.short_loss_multiplier
                    else:  # Long or flat
                        if alpha > 0:
                            reward += alpha * self.profit_scaling
                        else:
                            reward += alpha * self.loss_scaling

                    info['alpha'] = alpha
                    info['baseline_pnl'] = baseline_pnl
                    info['market_move'] = market_move_pips
                    info['reward_type'] = 'alpha_reward'
                else:
                    # Asymmetric PnL Scaling (fallback when alpha disabled)
                    if pnl_delta > 0:
                        reward += pnl_delta * self.profit_scaling
                    else:
                        reward += pnl_delta * self.loss_scaling
                    info['reward_type'] = 'continuous_pnl'
            else:
                # Sparse mode: track delta but don't reward until exit
                info['reward_type'] = 'sparse_holding'

            info['unrealized_pnl'] = current_unrealized_pnl
            info['pnl_delta'] = pnl_delta
            self.prev_unrealized_pnl = current_unrealized_pnl

            # Holding bonus - reward for staying in GROWING profitable trades
            # Only triggers if:
            # 1. Profit > 0.5x ATR (prevents camping on tiny profits)
            # 2. Profit is GROWING (pnl_delta > 0) (prevents camping when price stalls)
            min_profit_for_bonus = atr * 0.5  # Minimum 0.5x ATR profit required
            profit_is_growing = pnl_delta > 0
            if current_unrealized_pnl > min_profit_for_bonus and profit_is_growing and self.holding_bonus > 0:
                reward += self.holding_bonus
                info['holding_bonus_applied'] = True
            
            # Underwater Decay Penalty - penalizes holding losing trades
            # Encourages cutting losses early, teaching agent to manage drawdown
            if current_unrealized_pnl < 0 and self.underwater_penalty_coef > 0:
                # v28 FIX: Calculate raw price movement (not scaled by position_size)
                # This ensures penalty correctly measures ATR regardless of lot size
                current_price = self.close_prices[self.current_idx]
                if self.position == 1:  # Long
                    raw_pips_loss = (self.entry_price - current_price) / self.pip_value
                else:  # Short
                    raw_pips_loss = (current_price - self.entry_price) / self.pip_value
                
                # Calculate loss in ATR units (using raw price movement)
                loss_in_atr = max(0, raw_pips_loss) / max(atr, 1e-6)
                
                # Only penalize if beyond threshold (e.g., 0.5 ATR)
                if loss_in_atr > self.underwater_threshold_atr:
                    excess_loss = loss_in_atr - self.underwater_threshold_atr
                    # v31 FIX: Scale penalty by position_size to prevent size-up exploit
                    # Without this, agent could size up losers to dilute the fixed penalty
                    underwater_penalty = -self.underwater_penalty_coef * excess_loss * self.position_size
                    reward += underwater_penalty
                    info['underwater_penalty'] = underwater_penalty
                    info['loss_in_atr'] = loss_in_atr

        else:
            # Position is flat - ensure tracking is reset
            self.prev_unrealized_pnl = 0.0

        # Direction-Aware Opportunity Cost
        # Penalizes FLAT and WRONG-DIRECTION positions during significant moves
        # v41 only penalized flat → agent exploited by staying short 92% of time
        # v42 ensures being in wrong direction is also penalized
        if self.fomo_lookback_bars > 0:
            lookback_start = max(0, self.current_idx - self.fomo_lookback_bars)
            if lookback_start < self.current_idx:
                price_at_lookback = self.close_prices[lookback_start]
                current_price_fomo = self.close_prices[self.current_idx]
                multi_bar_move = current_price_fomo - price_at_lookback  # SIGNED move
                abs_move = abs(multi_bar_move)

                if abs_move > self.fomo_threshold_atr * atr:
                    # Determine if position is aligned with move direction
                    move_up = multi_bar_move > 0
                    wrong_direction = (
                        self.position == 0 or  # Flat during move
                        (move_up and self.position < 0) or  # Short during up move
                        (not move_up and self.position > 0)  # Long during down move
                    )

                    if wrong_direction:
                        if self.current_idx > 0:
                            prev_price = self.close_prices[self.current_idx - 1]
                            this_bar_move = abs(current_price_fomo - prev_price)
                            missed_profit_pips = this_bar_move / self.pip_value
                            opportunity_cost = -min(
                                missed_profit_pips * self.reward_scaling * self.opportunity_cost_multiplier,
                                self.opportunity_cost_cap
                            )
                            reward += opportunity_cost
                            info['fomo_triggered'] = True
                            info['fomo_move_pips'] = abs_move
                            info['fomo_opportunity_cost'] = opportunity_cost
                            info['fomo_wrong_direction'] = True

        # Chop penalty: holding position in ranging market
        if self.position != 0 and chop > self.chop_threshold:
            reward += self.chop_penalty
            info['chop_triggered'] = True

        return reward, info

    def reset(
        self,
        seed: Optional[int] = None,
        options: Optional[dict] = None
    ) -> Tuple[np.ndarray, dict]:
        """Reset the environment for a new episode."""
        super().reset(seed=seed)

        # Random starting point
        if options and 'start_idx' in options:
            self.current_idx = options['start_idx']
        elif self.use_regime_sampling and self.regime_indices is not None:
            # REGIME-BALANCED SAMPLING: Equal probability for each regime
            # This prevents directional bias from unbalanced training data
            available_regimes = [r for r in [0, 1, 2] if len(self.regime_indices[r]) > 0]
            if len(available_regimes) > 0:
                # Randomly pick a regime
                chosen_regime = self.np_random.choice(available_regimes)
                # Randomly pick a starting index from that regime
                regime_idx = self.np_random.integers(0, len(self.regime_indices[chosen_regime]))
                self.current_idx = self.regime_indices[chosen_regime][regime_idx]
            else:
                # Fallback to random if no regime indices available
                max_start = max(self.start_idx + 1, self.end_idx - self.max_steps)
                self.current_idx = self.np_random.integers(self.start_idx, max_start)
        else:
            # FIXED: Ensure valid range for random start
            max_start = max(self.start_idx + 1, self.end_idx - self.max_steps)
            self.current_idx = self.np_random.integers(
                self.start_idx,
                max_start
            )

        # Reset state
        self.position = 0
        self.position_size = 0.0
        self.entry_price = 0.0
        self.steps = 0
        self.total_pnl = 0.0
        self.trades = []
        self.prev_unrealized_pnl = 0.0  # Reset for continuous PnL tracking
        self._holding_bonus_paid = 0.0
        self._holding_bonus_level = 0
        self._profit_high_water_mark = 0.0
        self.break_even_activated = False
        self.entry_atr = 0.0

        # Warmup rolling window normalization with historical data
        # OPTIMIZED: Use vectorized numpy operations instead of per-element loops
        if self.use_rolling_norm and len(self.market_features.shape) > 1:
            # Reset circular buffer state
            self.rolling_buffer.fill(0)
            self.rolling_idx = 0
            self.rolling_count = 0
            self.rolling_sum.fill(0)
            self.rolling_sum_sq.fill(0)
            
            # Collect all warmup data
            warmup_chunks = []
            
            # PRIORITY 1: Use injected lookback data from BEFORE start_idx
            if self.rolling_lookback_data is not None and len(self.rolling_lookback_data) > 0:
                warmup_chunks.append(self.rolling_lookback_data)
            
            # PRIORITY 2: Also use data from within current env range (before current_idx)
            warmup_start = max(0, self.current_idx - self.rolling_window_size)
            if warmup_start < self.current_idx:
                warmup_chunks.append(self.market_features[warmup_start:self.current_idx])
            
            # Combine and fill buffer (take last rolling_window_size samples)
            if warmup_chunks:
                all_warmup = np.concatenate(warmup_chunks, axis=0) if len(warmup_chunks) > 1 else warmup_chunks[0]
                # Take last rolling_window_size samples
                warmup_data = all_warmup[-self.rolling_window_size:] if len(all_warmup) > self.rolling_window_size else all_warmup
                n_warmup = len(warmup_data)
                
                # Fill circular buffer
                self.rolling_buffer[:n_warmup] = warmup_data
                self.rolling_idx = n_warmup % self.rolling_window_size
                self.rolling_count = n_warmup
                
                # Compute running sums
                self.rolling_sum = warmup_data.sum(axis=0).astype(np.float64)
                self.rolling_sum_sq = (warmup_data ** 2).sum(axis=0).astype(np.float64)

        return self._get_observation(), {}

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, dict]:
        """
        Execute one step in the environment.

        Args:
            action: [direction, size] array

        Returns:
            observation, reward, terminated, truncated, info
        """
        # FIRST: Check stop-loss/take-profit BEFORE agent action
        # This enforces risk management regardless of what the agent wants to do
        sl_tp_reward, sl_tp_info = self._check_stop_loss_take_profit()

        # v23.1 FIX: If SL/TP triggered (trade closed), skip agent action this step
        # This prevents the "immediate re-entry" bug where agent can re-open a position
        # in the same step that SL/TP closed it, effectively bypassing risk management.
        # The agent must wait until the NEXT step to make a new decision.
        if sl_tp_info.get('trade_closed'):
            action_reward = 0.0
            action_info = {'action_skipped_sl_tp': True, 'intended_action': action.tolist()}
        else:
            # THEN: Execute agent's action (which may open new positions or do nothing)
            action_reward, action_info = self._execute_action(action)

        # Combine rewards and info
        reward = sl_tp_reward + action_reward
        info = {**action_info, **sl_tp_info}  # SL/TP info takes precedence

        # Store bars_held BEFORE index increment (for episode-end forced close)
        # This fixes the off-by-one bug where bars_held was calculated after increment
        bars_held_before_increment = self.current_idx - self.entry_idx if self.position != 0 else 0

        # Move to next step
        self.current_idx += 1
        self.steps += 1

        # Check termination
        terminated = self.current_idx >= self.end_idx
        truncated = self.steps >= self.max_steps

        # EPISODE BOUNDARY FIX:
        # If the episode ends with an open position, force-close it so the final
        # mark-to-market PnL delta is realized and trade statistics are complete.
        if (terminated or truncated) and self.position != 0:
            exit_idx = min(self.current_idx, len(self.close_prices) - 1)
            exit_price = float(self.close_prices[exit_idx])
            pip_value = self.pip_value

            if self.position == 1:  # Long
                pnl_pips = (exit_price - self.entry_price) / pip_value
            else:  # Short
                pnl_pips = (self.entry_price - exit_price) / pip_value

            final_unrealized = pnl_pips * self.position_size
            final_delta = final_unrealized - self.prev_unrealized_pnl

            # v24 FIX: In sparse mode, reward FULL trade PnL
            if self.use_sparse_rewards:
                forced_close_reward = final_unrealized * self.reward_scaling
            else:
                # v32 FIX: Apply alpha to forced close reward
                if self.use_alpha_reward and self.current_idx > 0:
                    current_price = self.close_prices[self.current_idx]
                    prev_price = self.close_prices[self.current_idx - 1]
                    market_move_pips = (current_price - prev_price) / self.pip_value
                    # v41 FIX: Directional baseline (long→B&H, short→S&H)
                    if self.position == 1:
                        exit_baseline = market_move_pips * self.alpha_baseline_exposure * self.position_size
                    else:
                        exit_baseline = -market_move_pips * self.alpha_baseline_exposure * self.position_size
                    exit_alpha = final_delta - exit_baseline
                    # Direction-compensated rewards
                    if self.position == -1:  # Short position
                        if exit_alpha > 0:
                            forced_close_reward = exit_alpha * self.profit_scaling * self.short_profit_multiplier
                        else:
                            forced_close_reward = exit_alpha * self.loss_scaling * self.short_loss_multiplier
                    else:  # Long position
                        if exit_alpha > 0:
                            forced_close_reward = exit_alpha * self.profit_scaling
                        else:
                            forced_close_reward = exit_alpha * self.loss_scaling
                else:
                    forced_close_reward = final_delta * self.reward_scaling
            reward += forced_close_reward

            self.total_pnl += final_unrealized
            self.trades.append({
                'entry': self.entry_price,
                'exit': exit_price,
                'direction': self.position,
                'size': self.position_size,
                'pnl': final_unrealized,
                'bars_held': bars_held_before_increment,  # FIX: Use pre-increment value
                'close_reason': 'episode_end'
            })

            # Reset position state
            self.position = 0
            self.position_size = 0.0
            self.entry_price = 0.0
            self.prev_unrealized_pnl = 0.0
            self._holding_bonus_paid = 0.0
            self._holding_bonus_level = 0
            self._profit_high_water_mark = 0.0
            self.break_even_activated = False
            self.entry_atr = 0.0

            info['episode_end_forced_close'] = True
            info['episode_end_forced_close_reward'] = forced_close_reward
            info['episode_end_forced_close_pnl'] = final_unrealized

        # Get new observation (guarded for end-of-data)
        # v26 FIX: Always return real observation, even at episode end
        # SB3 needs final observation for value bootstrapping on truncated episodes
        # Clamp index to valid range to avoid out-of-bounds
        safe_idx = min(self.current_idx, len(self.close_prices) - 1)
        self.current_idx = safe_idx  # Temporarily set safe index
        obs = self._get_observation()  # Get real final state

        # Add episode info
        info['step'] = self.steps
        info['position'] = self.position
        info['position_size'] = self.position_size
        info['entry_price'] = self.entry_price if self.position != 0 else None
        info['current_price'] = self.close_prices[min(self.current_idx, len(self.close_prices) - 1)]
        info['unrealized_pnl'] = self._calculate_unrealized_pnl()
        info['total_pnl'] = self.total_pnl
        info['n_trades'] = len(self.trades)

        # Market features for visualization
        if len(self.market_features.shape) > 1 and self.current_idx < len(self.market_features):
            mf = self.market_features[min(self.current_idx, len(self.market_features) - 1)]
            info['atr'] = float(mf[0]) if len(mf) > 0 else 0.0
            info['chop'] = float(mf[1]) if len(mf) > 1 else 50.0
            info['adx'] = float(mf[2]) if len(mf) > 2 else 25.0
            info['sma_distance'] = float(mf[3]) if len(mf) > 3 else 0.0

        # Analyst predictions
        if hasattr(self, 'current_probs') and self.current_probs is not None:
            info['p_down'] = float(self.current_probs[0])
            info['p_up'] = float(self.current_probs[-1])
            
        # Analyst activations (for visualization)
        if hasattr(self, 'current_activations') and self.current_activations is not None:
            info['analyst_activations'] = self.current_activations

        # Real OHLC data for visualization
        if self.ohlc_data is not None and self.current_idx < len(self.ohlc_data):
            ohlc = self.ohlc_data[self.current_idx]
            info['ohlc'] = {
                'open': float(ohlc[0]),
                'high': float(ohlc[1]),
                'low': float(ohlc[2]),
                'close': float(ohlc[3]),
            }
            if self.timestamps is not None and self.current_idx < len(self.timestamps):
                info['ohlc']['timestamp'] = int(self.timestamps[self.current_idx])

        # Pass trades list for win rate calculation (only on episode end to save memory)
        if terminated or truncated:
            info['trades'] = self.trades.copy()

        return obs, reward, terminated, truncated, info

    def render(self, mode: str = 'human'):
        """Render current state."""
        if mode == 'human':
            logger.info(f"Step: {self.steps}, Position: {self.position}, "
                        f"Size: {self.position_size:.2f}, PnL: {self.total_pnl:.2f} pips")

    def close(self):
        """Clean up resources."""
        del self._precomputed_contexts
        gc.collect()


def create_env_from_dataframes(
    df_15m: 'pd.DataFrame',
    df_1h: 'pd.DataFrame',
    df_4h: 'pd.DataFrame',
    analyst_model: Optional[torch.nn.Module] = None,
    feature_cols: Optional[list] = None,
    config: Optional[object] = None,
    device: Optional[torch.device] = None,
    noise_level = 0.05  # Moderate regularization noise
) -> TradingEnv:
    """
    Factory function to create TradingEnv from DataFrames.
    
    FIXED: 1H and 4H data now correctly subsampled from the aligned 15m index.

    Args:
        df_15m: 15-minute DataFrame with features
        df_1h: 1-hour DataFrame with features (aligned to 15m index)
        df_4h: 4-hour DataFrame with features (aligned to 15m index)
        analyst_model: Trained Market Analyst
        feature_cols: Feature columns to use
        config: TradingConfig object
        device: Torch device for analyst inference

    Returns:
        TradingEnv instance
    """
    import pandas as pd

    if feature_cols is None:
        feature_cols = ['open', 'high', 'low', 'close', 'atr',
                       'sma_distance']

    # Get default config values (5m/15m/45m system)
    # We keep legacy names in the signature for compatibility, but these now mean:
    #   df_15m -> 5m base
    #   df_1h  -> 15m mid
    #   df_4h  -> 45m high
    lookback_5m = 48
    lookback_15m = 16
    lookback_45m = 6

    if config is not None:
        # Support both new and legacy config field names
        lookback_5m = getattr(config, 'lookback_5m', getattr(config, 'lookback_15m', lookback_5m))
        lookback_15m = getattr(config, 'lookback_15m', getattr(config, 'lookback_1h', lookback_15m))
        lookback_45m = getattr(config, 'lookback_45m', getattr(config, 'lookback_4h', lookback_45m))

    # Subsampling ratios: how many 5m bars per higher TF bar
    subsample_15m = 3   # 3 x 5m = 15m
    subsample_45m = 9   # 9 x 5m = 45m

    # Calculate valid range - need enough indices for subsampled lookback
    start_idx = max(lookback_5m, lookback_15m * subsample_15m, lookback_45m * subsample_45m)
    n_samples = len(df_15m) - start_idx

    # Get feature arrays
    features_15m = df_15m[feature_cols].values.astype(np.float32)
    features_1h = df_1h[feature_cols].values.astype(np.float32)
    features_4h = df_4h[feature_cols].values.astype(np.float32)

    # Create windows for each timeframe
    data_15m = np.zeros((n_samples, lookback_5m, len(feature_cols)), dtype=np.float32)
    data_1h = np.zeros((n_samples, lookback_15m, len(feature_cols)), dtype=np.float32)
    data_4h = np.zeros((n_samples, lookback_45m, len(feature_cols)), dtype=np.float32)

    for i in range(n_samples):
        actual_idx = start_idx + i
        # 15m: direct indexing (includes current candle)
        data_15m[i] = features_15m[actual_idx - lookback_5m + 1:actual_idx + 1]

        # 15m mid timeframe: subsample every 3rd bar from aligned data
        idx_range_1h = list(range(
            actual_idx - (lookback_15m - 1) * subsample_15m,
            actual_idx + 1,
            subsample_15m
        ))
        data_1h[i] = features_1h[idx_range_1h]

        # 45m high timeframe: subsample every 9th bar from aligned data
        idx_range_4h = list(range(
            actual_idx - (lookback_45m - 1) * subsample_45m,
            actual_idx + 1,
            subsample_45m
        ))
        data_4h[i] = features_4h[idx_range_4h]

    # Close prices for PnL
    close_prices = df_15m['close'].values[start_idx:start_idx + n_samples].astype(np.float32)

    # Real OHLC data for visualization
    ohlc_data = None
    timestamps = None
    if all(col in df_15m.columns for col in ['open', 'high', 'low', 'close']):
        ohlc_data = df_15m[['open', 'high', 'low', 'close']].values[start_idx:start_idx + n_samples].astype(np.float32)
    if df_15m.index.dtype == 'datetime64[ns]' or hasattr(df_15m.index, 'to_pydatetime'):
        try:
            timestamps = (df_15m.index[start_idx:start_idx + n_samples].astype('int64') // 10**9).values
        except:
            pass  # Keep timestamps as None if conversion fails

    # Market features for reward shaping/observation.
    from src.live.bridge_constants import MARKET_FEATURE_COLS
    market_cols = list(MARKET_FEATURE_COLS)
    available_cols = [c for c in market_cols if c in df_15m.columns]
    market_features = df_15m[available_cols].values[start_idx:start_idx + n_samples].astype(np.float32)
    
    # Extract rolling lookback data (data BEFORE start_idx for warmup)
    rolling_window_size = default_config.normalization.rolling_window_size
    lookback_start = max(0, start_idx - rolling_window_size)
    rolling_lookback_data = df_15m[available_cols].values[lookback_start:start_idx].astype(np.float32)

    # Extract config values (defaults matching config/settings.py TradingConfig)
    pip_value = 1.0                 # US30: 1 point = 1.0 price movement
    spread_pips = 10.0              # config.trading.spread_pips
    fomo_penalty = 0.0              # DEPRECATED - now using opportunity cost
    chop_penalty = 0.0              # Disabled
    fomo_threshold_atr = 4.0        # config.trading.fomo_threshold_atr
    fomo_lookback_bars = 24         # config.trading.fomo_lookback_bars
    chop_threshold = 80.0           # config.trading.chop_threshold
    reward_scaling = 0.01           # config.trading.reward_scaling
    # Symmetric PnL Scaling
    profit_scaling = 0.01           # config.trading.profit_scaling
    loss_scaling = 0.01             # config.trading.loss_scaling
    # Alpha-Based Reward
    use_alpha_reward = True         # config.trading.use_alpha_reward
    alpha_baseline_exposure = 0.7   # config.trading.alpha_baseline_exposure
    holding_bonus = 0.0             # config.trading.holding_bonus (DEPRECATED)
    sl_atr_multiplier = 2.0         # config.trading.sl_atr_multiplier
    tp_atr_multiplier = 6.0         # config.trading.tp_atr_multiplier
    use_stop_loss = True            # config.trading.use_stop_loss
    use_take_profit = True          # config.trading.use_take_profit
    volatility_sizing = True
    risk_per_trade = 100.0          # config.trading.risk_per_trade
    enforce_analyst_alignment = False  # config.trading.enforce_analyst_alignment
    num_classes = 2
    # Early Exit Penalty defaults
    early_exit_penalty = 0.0        # config.trading.early_exit_penalty (DEPRECATED)
    min_bars_before_exit = 0        # config.trading.min_bars_before_exit
    rolling_norm_min_samples = 1

    # Opportunity Cost & Underwater defaults (matching config/settings.py)
    opportunity_cost_multiplier = 0.0  # config.trading.opportunity_cost_multiplier
    opportunity_cost_cap = 0.2      # config.trading.opportunity_cost_cap
    underwater_penalty_coef = 0.5   # config.trading.underwater_penalty_coef
    underwater_threshold_atr = 1.5  # config.trading.underwater_threshold_atr
    break_even_atr = 2.0            # config.trading.break_even_atr

    if config is not None:
        pip_value = getattr(config, 'pip_value', pip_value)
        spread_pips = getattr(config, 'spread_pips', spread_pips)
        fomo_penalty = getattr(config, 'fomo_penalty', fomo_penalty)
        chop_penalty = getattr(config, 'chop_penalty', chop_penalty)
        fomo_threshold_atr = getattr(config, 'fomo_threshold_atr', fomo_threshold_atr)
        fomo_lookback_bars = getattr(config, 'fomo_lookback_bars', fomo_lookback_bars)
        chop_threshold = getattr(config, 'chop_threshold', chop_threshold)
        reward_scaling = getattr(config, 'reward_scaling', reward_scaling)
        # Asymmetric PnL Scaling
        profit_scaling = getattr(config, 'profit_scaling', profit_scaling)
        loss_scaling = getattr(config, 'loss_scaling', loss_scaling)
        # Alpha-Based Reward
        use_alpha_reward = getattr(config, 'use_alpha_reward', use_alpha_reward)
        alpha_baseline_exposure = getattr(config, 'alpha_baseline_exposure', alpha_baseline_exposure)
        holding_bonus = getattr(config, 'holding_bonus', holding_bonus)
        sl_atr_multiplier = getattr(config, 'sl_atr_multiplier', sl_atr_multiplier)
        tp_atr_multiplier = getattr(config, 'tp_atr_multiplier', tp_atr_multiplier)
        use_stop_loss = getattr(config, 'use_stop_loss', use_stop_loss)
        use_take_profit = getattr(config, 'use_take_profit', use_take_profit)
        volatility_sizing = getattr(config, 'volatility_sizing', volatility_sizing)
        risk_per_trade = getattr(config, 'risk_per_trade', risk_per_trade)
        enforce_analyst_alignment = getattr(config, 'enforce_analyst_alignment', enforce_analyst_alignment)
        noise_level = getattr(config, 'noise_level', noise_level)
        # Early Exit Penalty
        early_exit_penalty = getattr(config, 'early_exit_penalty', -0.1)
        min_bars_before_exit = getattr(config, 'min_bars_before_exit', 10)
        rolling_norm_min_samples = getattr(config, 'rolling_norm_min_samples', rolling_norm_min_samples)
        
        # Opportunity Cost & Underwater Config
        opportunity_cost_multiplier = getattr(config, 'opportunity_cost_multiplier', opportunity_cost_multiplier)
        opportunity_cost_cap = getattr(config, 'opportunity_cost_cap', opportunity_cost_cap)
        underwater_penalty_coef = getattr(config, 'underwater_penalty_coef', underwater_penalty_coef)
        underwater_threshold_atr = getattr(config, 'underwater_threshold_atr', underwater_threshold_atr)
        break_even_atr = getattr(config, 'break_even_atr', break_even_atr)

    if analyst_model is not None:
        num_classes = getattr(analyst_model, 'num_classes', 2)

    return TradingEnv(
        # Map legacy arg names to TradingEnv signature
        data_5m=data_15m,
        data_15m=data_1h,
        data_45m=data_4h,
        close_prices=close_prices,
        market_features=market_features,
        analyst_model=analyst_model,
        lookback_5m=lookback_5m,
        lookback_15m=lookback_15m,
        lookback_45m=lookback_45m,
        device=device,
        # Config Params (US30)
        pip_value=pip_value,
        spread_pips=spread_pips,
        fomo_penalty=fomo_penalty,
        chop_penalty=chop_penalty,
        fomo_threshold_atr=fomo_threshold_atr,
        fomo_lookback_bars=fomo_lookback_bars,
        chop_threshold=chop_threshold,
        reward_scaling=reward_scaling,
        profit_scaling=profit_scaling,
        loss_scaling=loss_scaling,
        # Alpha-Based Reward
        use_alpha_reward=use_alpha_reward,
        alpha_baseline_exposure=alpha_baseline_exposure,
        holding_bonus=holding_bonus,
        sl_atr_multiplier=sl_atr_multiplier,
        tp_atr_multiplier=tp_atr_multiplier,
        use_stop_loss=use_stop_loss,
        use_take_profit=use_take_profit,
        volatility_sizing=volatility_sizing,
        risk_per_trade=risk_per_trade,
        enforce_analyst_alignment=enforce_analyst_alignment,
        num_classes=num_classes,
        # Visualization data
        ohlc_data=ohlc_data,
        timestamps=timestamps,
        noise_level=noise_level,
        # Early Exit Penalty
        early_exit_penalty=early_exit_penalty,
        min_bars_before_exit=min_bars_before_exit,
        # Rolling window warmup data
        rolling_lookback_data=rolling_lookback_data,
        rolling_norm_min_samples=rolling_norm_min_samples,
        # Opportunity Cost & Underwater
        opportunity_cost_multiplier=opportunity_cost_multiplier,
        opportunity_cost_cap=opportunity_cost_cap,
        underwater_penalty_coef=underwater_penalty_coef,
        underwater_threshold_atr=underwater_threshold_atr,
        break_even_atr=break_even_atr,
    )
